# nlp-benchmarks
Simple repo, where I want to keep track of reported task results on common NLP tasks, using different datasets

## Grouped by dataset/domain

- [BibSonomy ECML PKDD 2008](https://github.com/queirozfcom/nlp-benchmarks/blob/master/bibsonomy-ecml-pkdd-2008.md)

- [BibSonomy ECML PKDD 2009](https://github.com/queirozfcom/nlp-benchmarks/blob/master/bibsonomy-ecml-pkdd-2009.md)

- [CoNLL 2003 Named Entity Data](https://github.com/queirozfcom/nlp-benchmarks/blob/master/conll-2003-named-entity-data.md)

- [Delicious Datasets](https://github.com/queirozfcom/nlp-benchmarks/blob/master/delicious.md)

- [IMDB Dataset (Maas et al. 2011)](https://github.com/queirozfcom/nlp-benchmarks/blob/master/imdb.md)

- [Movie-review-data: Sentence Polarity (Pang and Lee 2005)](https://github.com/queirozfcom/nlp-benchmarks/blob/master/movie-review-data-sentence-polarity.md)

- [Stackexchange Data: Kaggle 2013](https://github.com/queirozfcom/nlp-benchmarks/blob/master/stackexchange-kaggle.md)

- [Stackoverflow Datasets]((https://github.com/queirozfcom/nlp-benchmarks/blob/master/stackoverflow.md))

- [Stanford Sentiment Treebank (Socher et al., 2013)](https://github.com/queirozfcom/nlp-benchmarks/blob/master/stanford-sentiment-treebank.md)
